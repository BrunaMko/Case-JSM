{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6265d6d1-a658-42dd-a06b-84e533a4c040",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "imports"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, to_timestamp, trim, lit, date_format,try_to_timestamp\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e30704-7f71-445c-9f2a-4a2e65805397",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CLASSE DE UTILITÁRIOS"
    }
   },
   "outputs": [],
   "source": [
    "class SilverSalesPipeline:\n",
    "    \"\"\"\n",
    "    Classe utilitária com a lógica de transformação e Upsert para a Camada Silver.\n",
    "    \"\"\"\n",
    "    def __init__(self, spark, silver_table_name: str):\n",
    "        self.spark = spark\n",
    "        self.silver_table_name = silver_table_name\n",
    "        \n",
    "        # Chaves primárias para o MERGE\n",
    "        self.primary_keys = [\"TransactionNo\", \"ProductNo\"]\n",
    "\n",
    "\n",
    "\n",
    "    def apply_transformations(self, bronze_df):\n",
    "        \"\"\"\n",
    "        Aplica limpeza, tipagem, metadados e filtros de qualidade.\n",
    "        \"\"\"\n",
    "        \n",
    "        silver_df = (\n",
    "            bronze_df\n",
    "            \n",
    "            # 1. Limpeza\n",
    "            .withColumn(\"Country\", trim(col(\"Country\")))\n",
    "            \n",
    "            # 2. Criação da coluna de preço total\n",
    "            .withColumn(\n",
    "                \"Total_Price\",\n",
    "                (col(\"Price\") * col(\"Quantity\").cast(\"int\"))\n",
    "                .cast(\"decimal(18,2)\")\n",
    "            )\n",
    "            # 3. CONVERSÃO DA DATA\n",
    "            .withColumn(\n",
    "                \"Transaction_Datetime\", \n",
    "                to_timestamp(col(\"Date\"), \"M/d/yyyy\")\n",
    "            )\n",
    "            # 4. Coluna de Metadado de Atualização\n",
    "            .withColumn(\"Update_Time_Silver\", current_timestamp())\n",
    "            \n",
    "            # Filtro Básico de Qualidade\n",
    "            .filter(col(\"CustomerNo\").isNotNull())\n",
    "            \n",
    "            # 4. Seleção final\n",
    "            .select(\n",
    "                col(\"TransactionNo\"),\n",
    "                col(\"CustomerNo\"),\n",
    "                col(\"Transaction_Datetime\"), \n",
    "                col(\"Price\"),\n",
    "                col(\"Total_Price\"),\n",
    "                col(\"Quantity\"),\n",
    "                col(\"Product\"),          \n",
    "                col(\"ProductNo\"),\n",
    "                col(\"Country\"),\n",
    "                col(\"Update_Time_Silver\"),\n",
    "                col(\"Ingestion_Time_Bronze\"), \n",
    "                col(\"Raw_Data_Source\")\n",
    "            )\n",
    "        )\n",
    "        return silver_df\n",
    "\n",
    "\n",
    "\n",
    "    def upsert_to_delta(self, batch_df, batch_id):\n",
    "        \"\"\"\n",
    "        Executa MERGE INTO na tabela Silver de forma transacional.\n",
    "        \"\"\"\n",
    "        print(f\"Processando lote {batch_id}\")\n",
    "        \n",
    "        # Deduplicação e verificação (mantido)\n",
    "        deduped_df = batch_df.dropDuplicates(self.primary_keys)\n",
    "        if deduped_df.isEmpty():\n",
    "            print(\"Lote vazio após deduplicação. Pulando MERGE.\")\n",
    "            return\n",
    "\n",
    "        deduped_df.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "        merge_query = f\"\"\"\n",
    "            MERGE INTO {self.silver_table_name} AS target\n",
    "            USING updates AS source\n",
    "            ON  target.TransactionNo = source.TransactionNo\n",
    "            AND target.ProductNo = source.ProductNo\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.CustomerNo = source.CustomerNo, \n",
    "                target.Transaction_Datetime = source.Transaction_Datetime,\n",
    "                target.Price = source.Price,\n",
    "                target.Total_Price = source.Total_Price,\n",
    "                target.Quantity = source.Quantity,\n",
    "                target.Product = source.Product,\n",
    "                target.ProductNo = source.ProductNo,\n",
    "                target.Country = source.Country,\n",
    "                target.Update_Time_Silver = source.Update_Time_Silver,\n",
    "                target.Ingestion_Time_Bronze = source.Ingestion_Time_Bronze,\n",
    "                target.Raw_Data_Source = source.Raw_Data_Source\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "                TransactionNo, CustomerNo, Transaction_Datetime, Price, Total_Price, Quantity, Product, ProductNo, Country, Update_Time_Silver, Ingestion_Time_Bronze, Raw_Data_Source\n",
    "            ) VALUES (\n",
    "                source.TransactionNo, \n",
    "                source.CustomerNo, \n",
    "                source.Transaction_Datetime,\n",
    "                source.Price,\n",
    "                source.Total_Price, \n",
    "                source.Quantity,\n",
    "                source.Product,\n",
    "                source.ProductNo, \n",
    "                source.Country,\n",
    "                source.Update_Time_Silver,\n",
    "                source.Ingestion_Time_Bronze, \n",
    "                source.Raw_Data_Source\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        self.spark.sql(merge_query)\n",
    "\n",
    "    def get_upsert_function(self):\n",
    "        \"\"\"\n",
    "        Retorna a função wrapper compatível com foreachBatch.\n",
    "        \"\"\"\n",
    "        def wrapper(batch_df, batch_id):\n",
    "            return self.upsert_to_delta(batch_df, batch_id)\n",
    "        return wrapper\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Pipeline_Utils_And_Silver_Setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}